{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f496ea564f10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmygrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftmax_crossentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmynn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0moptim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import mygrad as mg\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mynn.layers.dense import dense\n",
    "\n",
    "from mynn.activations.relu import relu\n",
    "from mynn.initializers.he_normal import he_normal\n",
    "\n",
    "\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "#from mynn.optimizers.sgd import SGD\n",
    "#optim = SGD(model.parameters, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, truth):\n",
    "    if isinstance(prediction, mg.Tensor):\n",
    "        prediction = prediction.data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__ (self, n, num_out):\n",
    "        \n",
    "        #n is internal i/o \n",
    "        #num out is vector\n",
    "        self.dense1 = dense(2, n, weight_initializer=he_normal)\n",
    "        self.dense2 = dense(n, n, weight_initializer=he_normal)\n",
    "        self.dense1 = dense(n, n, weight_initializer=he_normal)\n",
    "        self.dense2 = dense(n, num_out, weight_initializer=he_normal)\n",
    "    def __call__ (self, x):\n",
    "        return self.dense4(relu(self.dense3(relu(self.dense2(relu(self.dense1(x)))))))\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self.dense1.parameters+self.dense2.parameters+self.dense3.parameters+self.dense4.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])\n",
    "\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optim = SGD(model.parameters, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_cnt in range(5000):\n",
    "    idxs = np.arange(len(xtrain))  # -> array([0, 1, ..., 9999])\n",
    "    np.random.shuffle(idxs)  \n",
    "    \n",
    "    for batch_cnt in range(0, len(xtrain)//batch_size):\n",
    "        batch_indices = idxs[batch_cnt*batch_size : (batch_cnt + 1)*batch_size]\n",
    "        batch = xtrain[batch_indices]  # random batch of our training data\n",
    "\n",
    "        # `model.__call__ is responsible for performing the \"forward-pass\"\n",
    "        prediction = model(batch) \n",
    "        truth = ytrain[batch_indices]\n",
    "        \n",
    "        loss = softmax_crossentropy(prediction, truth)\n",
    "        acc = accuracy(prediction, truth)\n",
    "        \n",
    "        # you still must compute all the gradients!\n",
    "        loss.backward()\n",
    "        \n",
    "        # the optimizer is responsible for updating all of the parameters\n",
    "        optim.step()\n",
    "        loss.null_gradients()\n",
    "\n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_function(x):\n",
    "    from mygrad.nnet.activations import softmax\n",
    "    return softmax(model(x)).data\n",
    "\n",
    "fig, ax = spiral_data.visualize_model(dummy_function, entropy=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
